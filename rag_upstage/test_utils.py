from langchain_upstage import UpstageEmbeddings
from dotenv import load_dotenv
from langchain.schema.document import Document
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import PromptTemplate
from langchain_upstage import ChatUpstage
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader
import pandas as pd
import wikipediaapi
import re
import os
import itertools
import tiktoken


load_dotenv()
upstage_api_key = os.environ['UPSTAGE_API_KEY']
user_agent = os.environ['USER_AGENT']


def load_pdf(data_path):
    pdf_loader = PyPDFDirectoryLoader(data_path)
    documents = pdf_loader.load()
    return documents


#  í‚¤ì›Œë“œì˜ ì¡°í•©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±
def generate_subsets(keywords):
    subsets = []
    for i in range(len(keywords), 0, -1):  # í¬ê¸°ê°€ í° ê²ƒë¶€í„° ê²€ìƒ‰í•˜ë„ë¡ ìˆœì„œ ì¡°ì •
        subsets.extend(itertools.combinations(keywords, i))
    return [' '.join(subset) for subset in subsets]


def fetch_wiki_page(keyword, lang="en"):
    """
    Fetch a snippet from Wikipedia based on the query and summarize it using ChatUpstage.
    
    Parameters:
        keyword (str): The keyword to search in Wikipedia.
        lang (str): The language code for Wikipedia (default: 'en').
    
    Returns:
        str: A summarized text of the Wikipedia content if the page exists, otherwise None.
    """

    wiki_wiki = wikipediaapi.Wikipedia(user_agent, lang)
    keywords = keyword[0]['keywords']
    
    page_contents = []
    ###
    keywords = generate_subsets(keywords)
    ###
    for key in keywords:
        page = wiki_wiki.page(key)

        if page.exists():
            page_content = page.text
            document = Document(
                page_content=page_content,
                metadata={"title": page.title, "url": page.fullurl}
            )
            page_contents.append(document)
            print(f"âœ… Wikipedia page fetched for '{key}'")

        else:
            print(f"âŒ Wikipedia page not found for '{key}'")

    return page_contents


def extract_answer(response):
    """
    extracts the answer from the response using a regular expression.
    expected format: "(A) convolutional networks"

    if there are any answers formatted like the format, it returns None.
    """
    pattern = r"\([A-J]\)"  # Regular expression to find the first occurrence of (A), (B), (C), etc.
    match = re.search(pattern, response)

    if match:
        return match.group(0) # Return the full match (e.g., "(A)")
    else:
        return None


def accuracy(answers, responses):
    """
    Calculates the accuracy of the generated answers.
    
    Parameters:
        answers (list): The list of correct answers.
        responses (list): The list of generated responses.
    Returns:
        float: The accuracy percentage.
    """
    cnt = 0

    for answer, response in zip(answers, responses):
        print("-" * 10)
        generated_answer = extract_answer(response)
        print(response)

        # check
        if generated_answer:
            print(f"generated answer: {generated_answer}, answer: {answer}")
        else:
            print("extraction fail")

        if generated_answer is None:
            continue
        if generated_answer in answer:
            cnt += 1

    acc = (cnt / len(answers)) * 100

    return acc


from langchain_experimental.text_splitter import SemanticChunker
from langchain_upstage.embeddings import UpstageEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter


def split_documents(document: Document) -> list[Document]:
    """
    RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ë¬¸ì„œë¥¼ ë¶„í• .
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=100,
        length_function=len,
        is_separator_regex=False,
    )
    
    # ë‹¨ì¼ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• 
    chunks = text_splitter.split_text(document.page_content)

    # ê° chunkë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸°ë¥¼ metadataì— ì¶”ê°€
    chunked_documents = [
        Document(page_content=chunk, metadata={**document.metadata, "chunk_size": len(chunk)})
        for chunk in chunks
    ]
        
    return chunked_documents


def sem_split_documents(documents: list[Document], threshold: str) -> list[Document]:
    """
    SemanticChunkerë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³ , í¬ê¸°ê°€ í° chunkëŠ” ë‹¤ì‹œ ë¶„í• .
    """
    max_chunk_size = 1000
    max_token_size = 4000
    buffer_size = 500

    # SemanticChunker ì„¤ì •
    sem_text_splitter = SemanticChunker(
        embeddings=UpstageEmbeddings(
            model="solar-embedding-1-large-query", 
            api_key=upstage_api_key
        ),
        buffer_size=buffer_size,
        breakpoint_threshold_type=threshold
    )

    # ì²˜ìŒ ë¬¸ì„œ ë¶„í• 
    chunks = sem_text_splitter.split_documents(documents)

    # ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    final_chunks = []

    # ê° chunkì— ëŒ€í•´ í¬ê¸°ë¥¼ í™•ì¸í•˜ê³ , 1000 ì´ìƒì´ë©´ ë‹¤ì‹œ ë¶„í• 
    for chunk in chunks:
        # chunk, token ê¸¸ì´ ì¸¡ì •
        tokenizer = tiktoken.get_encoding("cl100k_base")
        token_size = len(tokenizer.encode(chunk.page_content))
        chunk_size = len(chunk.page_content)
        print(f"í˜„ì¬ chunk í¬ê¸°: {chunk_size}, í˜„ì¬ token í¬ê¸°: {token_size}")

        if chunk_size >= max_chunk_size:
            print(f"í¬ê¸°ê°€ í° chunk ë°œê²¬: {len(chunk.page_content)}. RecursiveCharacterTextSplitterë¡œ ì¬ë¶„í• í•©ë‹ˆë‹¤.")
            # RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•´ ì¬ë¶„í• 
            sub_chunks = split_documents(chunk)
            print(f"ë¶„í• ëœ chunk í¬ê¸°: {[len(sub_chunk.page_content) for sub_chunk in sub_chunks]}")
            final_chunks.extend(sub_chunks)
        else:
            final_chunks.append(chunk)

    return final_chunks


def extract_question_keywords(question):
    llm = ChatUpstage(api_key=upstage_api_key, model="solar-1-mini-chat")

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt_template = PromptTemplate.from_template(
        """
        You are a question analyzer. For the following multiple-choice question, perform the following tasks:
        
        1. Identify the problem type (e.g., "Math", "General Knowledge", "Legal", etc.).
        2. Extract the core question being asked.
        3. Extract 3-5 relevant keywords (each no more than 3 words) to answer the question effectively.

        Provide the output in JSON format:
        {{
            "problem_type": "[problem type]",
            "core_question": "[core question]",
            "keywords": ["keyword1", "keyword2", "keyword3", ...]
        }}

        ---
        Question:
        {question_text}
        """
    )

    # ì²´ì¸ ìƒì„±
    chain = prompt_template | llm
    results = []
    input_dict = {"question_text": question}
    response = chain.invoke(input_dict).content.strip()

    try:
        # ì‘ë‹µì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result_dict = eval(response)  # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            results.append(result_dict)
    except Exception as e:
        print(f"Error parsing response for prompt: {question}\nError: {e}")

    return results


def fetch_wiki_page2(keywords, lang='en'):

    wiki_wiki = wikipediaapi.Wikipedia(user_agent, lang)
    keywords = keywords[0]['keywords']
    
    page_contents = []
    ###
    keywords = generate_subsets(keywords)
    print(f"keywords list: {keywords}")
    ###
    for key in keywords:
        page = wiki_wiki.page(key)

        if page.exists():
            sections = page.sections
            for s in sections:
                if len(s.text) > 0:
                    document = Document(
                        page_content=s.text,
                        metadata={"title": s.title, "url": page.fullurl}
                    )
                    page_contents.append(document)
                    print(f"({len(s.text)}) : {s.title}")
            print(f"âœ… Wikipedia page fetched for '{key}'")
        else:
            print(f"âŒ Wikipedia page not found for '{key}'")

    print("âœ…âœ… Wikipedia page fetch completed!")
    return page_contents



def sem_split_documents2(
    documents: list[Document], 
    threshold: str, 
    split_count: int = 0, 
    max_splits: int = 1, 
    buffer_size: int = 500,
    is_recursive=False
) -> list[Document]:
    """
    SemanticChunkerë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ê³ , í¬ê¸°ê°€ í° chunkëŠ” ë‹¤ì‹œ ë¶„í• .
    ìµœëŒ€ ì¬ë¶„í•  íšŸìˆ˜ë¥¼ ì œí•œí•˜ë©°, ì¬ë¶„í•  ì‹œ buffer_sizeë¥¼ ì ì§„ì ìœ¼ë¡œ ì¤„ì„.
    """
    # SemanticChunker ì„¤ì •
    sem_text_splitter = SemanticChunker(
        embeddings=UpstageEmbeddings(
            model="solar-embedding-1-large-query", 
            api_key=upstage_api_key
        ),
        buffer_size=buffer_size,
        breakpoint_threshold_type=threshold
    )

    # ë¬¸ì„œ ë¶„í• 
    chunks = sem_text_splitter.split_documents(documents)

    # Initial chunk sizesëŠ” ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ ì¶œë ¥
    if not is_recursive:
        print(f"  ğŸ“œ Initial Splited chunk size: {[len(chunk.page_content) for chunk in chunks]}")

    # ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    final_chunks = []

    # ê° chunkì— ëŒ€í•´ í¬ê¸°ë¥¼ í™•ì¸í•˜ê³ , buffer_size ì´ìƒì´ë©´ ë‹¤ì‹œ ë¶„í• 
    for chunk in chunks:
        chunk_size = len(chunk.page_content)

        if chunk_size >= 1000:
            print(f"ğŸ‘‰ í¬ê¸°ê°€ í° chunk ë°œê²¬: {chunk_size}.")

            # ì¬ë¶„í•  íšŸìˆ˜ê°€ ìµœëŒ€ì¹˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
            if split_count >= max_splits:
                print(f"â›” ìµœëŒ€ ì¬ë¶„í•  íšŸìˆ˜({max_splits}) ë„ë‹¬: ë” ì´ìƒ ë¶„í• í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                final_chunks.append(chunk)
            else:
                # ìƒˆë¡œìš´ buffer_size ê³„ì‚°
                new_buffer_size = max(buffer_size // 2, 100)  # buffer_sizeì˜ ìµœì†Œê°’ì€ 500ìœ¼ë¡œ ì„¤ì •
                print(f"ğŸ”„ ì¬ë¶„í•  ì§„í–‰, ìƒˆë¡œìš´ buffer_size: {new_buffer_size}")

                # ì¬ë¶„í•  ì§„í–‰: split_countë¥¼ ì¦ê°€ì‹œì¼œ ì¬ê·€ í˜¸ì¶œ
                sub_chunks = sem_split_documents2(
                    [chunk], 
                    threshold, 
                    split_count + 1, 
                    max_splits, 
                    buffer_size=new_buffer_size,
                    is_recursive=True
                )
                final_chunks.extend(sub_chunks)
                print(f"  ğŸ”¹ Sub-chunk sizes after split: {[len(sub_chunk.page_content) for sub_chunk in sub_chunks]}")
        else:
            final_chunks.append(chunk)

    # ìµœì¢… ì²­í¬ í¬ê¸° ì¶œë ¥
    if not is_recursive:
        print(f"  âœ… Final chunk sizes: {[len(chunk.page_content) for chunk in final_chunks]}")

    return final_chunks


def extract_options(text):
    # "(A)"ë¶€í„° ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì„ ì°¾ê³ , ê·¸ ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
    start_idx = text.find("(A) ")  # "(A)"ì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
    
    if start_idx == -1:
        return "No options found"
    
    # "(A)" ì´í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸°
    options_part = text[start_idx:].lstrip()
    
    return options_part